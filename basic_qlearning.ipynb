{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = ('U','D','L','R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Grid: # Environment\n",
    "    def __init__(self,rows,cols,start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i,j):r   (row,coll):reward\n",
    "        # actioms should be a dict of: (i,j):a   (row,coll):action\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "    \n",
    "    def set_state(self,s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return (self.i,self.j)\n",
    "    \n",
    "    def is_terminal(self,s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def get_next_state(self,s,a):\n",
    "        i,j = s[0], s[1]\n",
    "        if a in self.actions[(i,j)]:\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            elif a == 'D':\n",
    "                i += 1\n",
    "            elif a == 'R':\n",
    "                j += 1\n",
    "            elif a == 'L':\n",
    "                j -=1\n",
    "        return i,j\n",
    "    \n",
    "    def move(self,action):\n",
    "        self.i,self.j  = self.get_next_state((self.i,self.j),action)\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "    \n",
    "    \n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true is we aare in a state whe no actions are possible\n",
    "        return (self.i,self.j) not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys())|set(self.rewards.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    # define a  grid that describes the reward for arriving at each state\n",
    "    # and possibe actions at each state\n",
    "    # the grid looks like this \n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at the state\n",
    "    # . . .  1\n",
    "    # . x . -1\n",
    "    # s . .  . \n",
    "    g = Grid(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0):('D','R'),\n",
    "        (0,1):('R','L'),\n",
    "        (0,2):('D','R','L'),\n",
    "#         (0,3):('D','U','R','L') Terminal state +1\n",
    "        (1,0):('D','U'),\n",
    "#         (1,1):('D','U','R','L'), Wall\n",
    "        (1,2):('D','U','R'),\n",
    "#         (0,0):('D','U','R','L') Terminal state -1\n",
    "        (2,0):('U','R'),\n",
    "        (2,1):('R','L'),\n",
    "        (2,2):('U','R','L'),\n",
    "        (2,3):('U','L')\n",
    "    }\n",
    "    g.set(rewards,actions)\n",
    "    return g \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_grid(step_cost = -0.1):\n",
    "    # in this version we try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGrid(Grid):\n",
    "    def __init__(self,rows,cols,start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    \n",
    "    def set(self,rewards,actions,probs):\n",
    "        # rewards should be a dict of: (i,j): r\n",
    "        # actions should be a dict of: (i,j): A   A is list of action\n",
    "        self.rewards = rewards \n",
    "        self.actions = actions\n",
    "        self.probs = probs\n",
    "        \n",
    "    \n",
    "    def move(self,action):\n",
    "        s = (self.i,self.j)\n",
    "        a = action\n",
    "        next_state_probs = self.probs[(s,a)]\n",
    "        next_states = list(next_state_probs.keys())\n",
    "        s = np.random.choice(next_states,p  = next_probs)\n",
    "        \n",
    "        # update the current state\n",
    "        self.i,self.j = s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def windy_grid():\n",
    "    g = WindyGrid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "    }\n",
    "\n",
    "    # p(s' | s, a) represented as:\n",
    "    # KEY: (s, a) --> VALUE: {s': p(s' | s, a)}\n",
    "    probs = {\n",
    "    ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "    ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "    ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "    ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "    ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "    ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "    ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "    ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "    ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "    ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "    ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "    ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "    ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "    ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    g.set(rewards, actions, probs)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Print vlaues and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max dict and random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    # put this into a function since we are using it so often\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    # we'll use epsilon-soft to ensure all states are visited\n",
    "    \n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9 # Discount Factor\n",
    "ALPHA = 0.1 # Learning Rate\n",
    "ACTION_SPACE = ('U','D','L','R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = negative_grid(step_cost = -0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n"
     ]
    }
   ],
   "source": [
    "print_values(grid.rewards,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ACTION_SPACE:\n",
    "        Q[s][a] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (1, 3): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (2, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (2, 3): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (1, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (0, 3): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (0, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (1, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (2, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (2, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0},\n",
       " (0, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping track of how many times  Q(s,a) has been updated\n",
    "update_counts = {}\n",
    "update_counts_sa = {}\n",
    "for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ACTION_SPACE:\n",
    "        update_counts_sa[s][a] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    }
   ],
   "source": [
    "# repeat until convergence\n",
    "t = 1.0\n",
    "deltas = []\n",
    "\n",
    "N = 10000\n",
    " \n",
    "for it in range(N):\n",
    "    if it % 100 == 0:\n",
    "        t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "        print('it:',it)\n",
    "    \n",
    "    # instead of generating an episode, we will PLAY \n",
    "    # an episode within this loop\n",
    "    s = (2,0) # start state\n",
    "    grid.set_state(s)\n",
    "    \n",
    "    # the first (s,r) tuple is the state we start in and 0 \n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # tge last (s,r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't \n",
    "    # care about updating it.\n",
    "    \n",
    "    a,_ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over(): # While not game_over take an action...\n",
    "        a = random_action(a, eps = 0.5/t) # epsilon-greedy aproach\n",
    "        # random action also works, but it is slower since it can bump into walls\n",
    "        # a = np.random.choice(ACTION_SPACE)\n",
    "        r = grid.move(a)\n",
    "        s2 = grid.current_state()\n",
    "        \n",
    "        # we will update Q(s,a) AS we experience the episode\n",
    "        old_qsa = Q[s][a]\n",
    "        # the difference between SARSA and q-learning is with q-learning\n",
    "        # we will use the max[a']{Q(s',a')} in our update\n",
    "        # even if we do not end up taking this action in the next step\n",
    "        a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "        \n",
    "        \n",
    "        # actual q-learning step\n",
    "        Q[s][a] = Q[s][a]+ALPHA*(r + GAMMA * max_q_s2a2 - Q[s][a])\n",
    "        \n",
    "        biggest_change = max(biggest_change,np.abs(old_qsa-Q[s][a]))\n",
    "    \n",
    "        # we would like to know how often Q(s) has been updated too\n",
    "        update_counts[s] = update_counts.get(s,0)+1\n",
    "        \n",
    "        # next states become current state\n",
    "        s = s2\n",
    "        a = a2\n",
    "    \n",
    "    deltas.append(biggest_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the changes in q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(deltas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the policy p* from Q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find V* from Q*\n",
    "policy = {}\n",
    "V = {}\n",
    "for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### what's the proportion of time we spend updating each part of Q?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.18| 0.18| 0.18| 0.00|\n",
      "---------------------------\n",
      " 0.19| 0.00| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.19| 0.02| 0.02| 0.00|\n"
     ]
    }
   ],
   "source": [
    "print(\"update counts:\")\n",
    "total = np.sum(list(update_counts.values()))\n",
    "for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "print_values(update_counts, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Policy and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "print(\"values:\")\n",
    "print_values(V, grid)\n",
    "print(\"policy:\")\n",
    "print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
